2018

19 Jan
Most of the day was spent trying to get conjoint_report from qconjoint working correctly. Made some progress. Java seems to be running but the pptx function does not work still. More details later. I also worked on the powerpoint conjoint report for Centene. I don't like powerpoint as a medium of communication but I understand that clients like it. Lots of work needs to go into the report design and automation. I like designing things but not that much. Needs new typography. Up to this point, I've spent most of my time trying to resolve computer issues. I'm hoping they get resolved soon. I need to develop goals for the next month. 

22 Jan
I started the day by creating a conjoint simulator in Excel---basically a
table that lets you choose factor levels from a drop down menu and compares
different packages to each other based on potential market share. Then I ran a
maxdiff analysis for an Uber client because their data was too large to run in
the online maxdiff analyzer. I want to automate the building of a simulator
but I think that will be difficult in python. R shiny seems to be a more
appropriate tool to use for any sort of interactive application. I'd like to
work on automating several processes: 1) slideshow summary report 2)
spreadsheet simulator 3) data preprocessing 4) conjoint and maxdiff pipelines.
Also looking forward to building in optimization processes for current
analyses.

23 Jan
Learned Qualtrics survey design UI for a Coursera price study. Began studying
ways of improving a TURF analysis for 40 features where the best combo of the
top 7 features will be needed. Parallelizing the algorithm will likely not
solve the problem because of memory issues. An alternative solution would be
to process the algorithm in batches. Alternatively, another model could be
used such as a genetic algorithm to find the best combination. 40 choose 7 is
pretty large but should be manageable.

24 Jan
Learned how to access the PHP databases where survey data and analyses are
stored. Also learned how to customize the web app for the surveys. Concrete
example: I learned how to change the price variable from being a string or
interval to being a randomly generated integer within an interval so the
survey will show the randomly drawn value instead of some fixed number. I also
learned how to generate a design by specifying a csv of attributes and levels
for Campos-Rinovum conjoint. Used the Qualtrics web app UI to generate a
design matrix and survey. Changed the color theme of the survey conjoint
questions. Also, as I've been working on ways to improve TURF for OPTUM, I'm
increasingly convinced that the standard TURF algorithm is very poor. It
answers a poorly specified question and very easily runs into errors or
unstable results. My thoughts on improving the analysis for OPTUM are to use a
genetic algorithm instead or at least perform some form of segmentation on the
data before it goes through analysis. I have yet to walk through this with
Craig and JD to see what they think. However, the biggest problem is the
specification of an objective function. It seems like their goal is to
optimize something that doesn't make very much sense to me. Clarifying the
objective would help me know which algorithm to use. In the worst case, we
should just be able to run the standard TURF implementation in R using the
mcmc argument to save on memory. It ran quickly on my computer for 600 rows
and 40 variables with randomly generated data. However, there is no guarantee
of getting the global maximum or even consistent results. Despite this, TURF
analysis doesn't seem to guarantee much along those lines anyway even if we
were to run the full analysis.

25 Jan
Prepared a summary report (powerpoint) for Lending Club which from what I
understood from Craig is re-running their analysis. I also updated part of the
html for Coursera's online survey. Talked to JD about how to approach TURF
analysis. He proposed splitting the data into batches, running each batch
individually, and combining the results to get the top packages. He also
agreed that the output from TURF analysis is probably not helpful for the
client. He agreed with my idea that it would be a good idea to consider developing an
alternative analysis that might be more helpful to the client. I don't know if
Craig will agree to that but I will propose it when the time gets closer to
the actual analysis. In the meantime, I feel my time is best spent working on
automating the summary reports using LaTeX instead of powerpoint. LaTeX is
open source and doesn't have massive updates or require subscription licenses
so I feel it is a good alternative to powerpoint. Of course, the client may
disagree but I believe that they will not care if it is a pdf or a powerpoint
as long as it is in a slideshow-style format. I'm also working on crafting the
right language for the report that is more clear and to the point. I also need
to develop a consistent and good data visualization style for the reporting,
which I will probably end up doing in Python despite R's generally superior
visualization tools. Another addition that may be possible to build into the
pdf is the excel simulator. If the pdf is interactive, that could make the
report a one-size-fits-all solution. We could have a standard offering which
is a basic report and then features we could add such as explanation of the
methods we used and the interactive simulator. All this would be automatically
generated of course.

26 Jan
Ran a maxdiff analysis for OICremo. Along with that, I used my dataset.utils
function to automatically load the maxdiff data from the raw survey data. I
improved the function so it's pretty robust but not fully fool-proof. I also
need to work on the gen_design function. Also important to note: I noticed
that there may be something wrong with the way pandas is handling the csvs.
After loading a dataframe, it somehow deleted the dataset or something. The
file was blank when I tried to open it in excel. Maybe I should start using
python scripts. I thought some more about modifying the latex report style but
I didn't get around to selecting alternative visualizations for the output of
the analyses. JD and I talked about writing a shell script to automatically
pipe the raw data into my dataset cleaning functions and then pipe the output
of those into an R script for analysis. The shell script should be a good way
to define our entire pipeline. I met Jeff Dotson, a BYU marketing professor.
He was unexpectedly rigorous in a quantitative way; he is well trained
analytically and quantitatively. He mentioned some good ideas about Thompson
Sampling for adaptive conjoint/maxdiff analysis. In addition he is working on
trying ensemble methods for conjoint to improve conjoint prediction which I
think is a cool idea. He also mentioned how he's trying to find a good way to
use random forests for conjoint data but the structure of conjoint data makes
it difficult to implement for a random forest algorithm. I want to look more
into that but we will see what time allows during or after work. Jeff will be
a great resource going forward and should be a great outside consultant when
needed for advice or opinions. He also seems to have respect and sway with
Qualtrics. He and I seem to have similar background and knowledge so I may be
able to use him to influence opinions if my own attempts don't go over so
well.

29 Jan
Completed building a survey for Campos-Rinovum. Also met and discussed the
report design with JD. We talked about overall goal of the report and what is
most useful to the client. JD is assigned to work on the data visualization
redesigns. I am assigned to work on generating a color scheme, the fake data
to use for analysis, and everything else for the report that is not the data
visualizations. The goal is to first create a dummy report for conjoint
analysis that we can send to clients to give them an idea of what they will
receive for the conjoint analysis. Then we will work on automating that report
so that it builds as the conjoint analysis is performed. Remember that a
maxdiff dummy report should be built as well. Team meeting (Craig, JD).
Discussed the metrics we need to track. In the past we tracked billing only
pretty much. Now in addition we need to track burndown (how much did the
project make when all is said and done?), Utilization (time spent on client
projects as a percentage of the number of hours worked in a week, goal of
around 70%), number of hours worked on a client project (on a per project
basis, recorded in Trello), and CSAT surveys to clients. I am particularly
responsible to get the last one started every Friday. We will also be meeting
on Thursday this week to begin working on the STAN HB estimation model to make
it better. I feel like I need to know HB model estimation like the back of my
hand so I'll be working on that on my own time.

30 Jan
Worked on a survey for the Center for Emotional Marketing - Toms, basic setup.
Also unit tested a survey that Craig built for MLB (GG price optimization?).
Started work on a BMW report that combines the summary reports from the US and
Germany. It's messy but I'm just getting it done. PowerPoint is a real waste
of my time. Formatting is very difficult--especially with the types of graphs
we have. It would be much better if the graphs were pictures. Of course,
that's why I'm hoping for InDesign so that we save the graphs as images and
embed them into a standard pdf slide presentation. I worked on the reporting
for a bit today--mostly on thinking about the dummy data. I started modifying
the Centene data to use for the dummy report. I really want to see if our
clients care about the format of the final report. It's just not consistent
across platforms in pptx. My goal is to get that data running in a well
designed dummy report. I submitted an IT ticket to get access to InDesign. IT
wrote back and asked why I needed InDesign. I hope I don't have to fight too
hard to get it. I still feel the lack of a development process or workflow. It
seems very ad hoc. It could be because the team is fairly new. I hope we can
focus on future developments that help us speed things up and make our
processes more efficient and reproducible.
